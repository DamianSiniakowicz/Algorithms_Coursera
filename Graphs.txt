Randomized Select

CHECK : RSA 20:13 Master Method for lower bound	



`````````````````

Graphs : pairwise relationships between nodes/vertices. Relationships are called edges. Undirected and directed graphs. 
In directed, nodes are ordered. Directed edges also called arcs. tails and edges of vertices. fully connected graph has 2^n cuts

Complete Graph : a graph where every node is connected to all other nodes

Degree : the average number of edges per node

Cut :  a grouping of vertices into two non-empty sets

Crossing Edges : Edges that have endpoints in both sets. For directed graphs, we're sometimes only interested in crossing edges from A to B.

Min-Cuts : the cut with the fewest crossing edges. Used in graph partitioning (analyze social networks for social conglomerates, identify stressed / vulnerable points in infrastructure). Can be calculated for weighted edges, too, (in image segmentation)

Sparse Graphs : number of edges close to number of nodes
Dense Graphs : number of edges close to square of number of nodes

Representations (Adjacency Lists)
~ Adjacency Matrix (ok for dense graphs) : n^2 space n by n matrix. Can represent directed and undirected graphs, with parallel edges.
~ Adjacency Lists (good for sparse graphs) : m + n space. List of edges, each edge has a pointer to its vertices (2m space). List of vertices, each vertex has a list of edges it's involved in (n + m space). Total is (n + 3m space). Great for graph search 

Randomized Contraction :
~ Choose two vertices, eliminate all edges connecting them, and fuse them into one vertex. Edges connecting either node participating in the fusion to outside nodes are now connected to the fused node. 
~ Keep doing this until you have just 2 vertices remaining.
~ the remaining edges are your cuts. (Imp : don't "fuse" the nodes, just put them into sets. All you need to update is the list of edges to be cut. Essentially we are saving edges from the knife with each contraction. Saving one parallel edge saves both.)

Probability of Randomized Contraction Computing a Particular Min Cut: >= 1/n^2
~ = probability of never contracting a crossing edge. 
~ degree of graph is >= k, the size of the min cut. Sum of degrees is equal to twice the number of edges. 	 

Counting min-cuts : n choose 2 mincuts for a graph with n vertices

`````

RC implementation

min_cut_hw = read.table("~/Downloads/min_cut.txt",fill=TRUE,header=FALSE,col.names=(paste(1:40,"var")))

0.) Make a copy of the vertex neighbor lists
1.) make a set of sets. Each set contains a vertex. 
2.) Choose an edge by randomly choosing a row in the vertex lists, then randomly choosing a column in that row. if the val is NA, re-choose a row/col
3.) Merge the member vertices into one set in your set of sets. 
4.) gray out the corresponding entries in the vertex neighbors list for both of the fused vertices. (by grey out I mean replace with NA)

5.) Check if there are only two sets remaining. If so, we're done.



#####################################################################################################################################################

Week 4

$$$$$ Notes $$$$$

Applications 
~ checking if graph is fully connected
~ finding shortest path
~ finding an ordered list of actions that will take you from start state to goal state. states = vertices & actions = edges
~ compute "pieces" and "components" of a graph. the clusters / structure

Generic Graph Search
~ get list of all vertices you can get to from a "source vertex." These are the 'findable vertices'
~ goal O(m + n)
~ algo : maintain a list of explored nodes. choose an edge that contains one explored and one unexplored node.
~ BFS and DFS provide two distinct rules for choosing the next node to explore.
~ Both O(m + n) time
~ DFS great for connected components

BFS
~ O(2m + n) time
~ keep a list of explored nodes

BFS and Shortest Paths
~ store each node in the queue together with its layer. ex. in a dict / list / tuple / struct / etc.
~ you can thus return the distance of the shortest path
~ guaranteed to find the shortest path

BFS and Undirected Connectivity
~ Connected Components : connected pieces of a graph. Formally : equivalence classes where the relation is : there exists a path between
~ Goal : see if graph is connected
~ ex. construct a graph where nodes are connected if they are 'similar' enough, if enough money is flowing between them, etc. then cluster
~ algo : list of explored nodes, connected components integer count. loop over all nodes: if unexplored do BFS to explore it's connected component, add the nodes you explore to the explored nodes list and increment count by 1; if unexplored then look at the next node in the node loop.
~ O(m + n)
~ DFS is also O(m + n) for connected components on an undirected graph

DFS
~ good for topological ordering in directed, acyclic graphs and for strongly connected components of directed graphs
~ O(m + n)

DFS & Topological Sort
~ Ordering : an ordering of the vertices such that all arcs go forward
~ useful for problems with precedence constraints, ex. planning out courses for a major
~ must be acyclic
~ Sink Vertex : no outgoing arc. only happen in directed, acyclic graphs.
~ algo : put sink in last position - 1 , remove all edges leading to it and recurse on rest of graph. count down from n to 1
~ dfs algo : explored nodes list = [], current_label = n, node to label mapping = {}. loop over all nodes: if explore already then skip; if not explored then call DFS on it and map the current_label to the node that DFS terminates on, also decrement current_label by 1 and add all nodes explored by DFS to the explored_nodes_list.
~ O(m + n)

DFS & Strong Components
~ Strongly Connected Components : components of a DIRECTED graph where you can get from any node to any node. Same equivalence relation.
~ the component-wise graph can be acyclic even though the node-wise graph isn't.
~ DFS discovers SCC's, but it might get several SCC's or even the entire graph.
~ algo : run DFS loop on reversed graph (get some ordering of the nodes, finishing time(f_t=0, each time DFS ends on a node, set it to f_t and increment f_t)), run DFS on graph (on the ordering we got from the first loop, in decreasing order of finishing times. all newly discovered nodes with the same leader (the node from which the most recent DFS started) are an SCC).
~ summary : calculate DFS termination order (1 to n) on reverse graph, then call DFS on nodes in decreasing termination order(n to 1) using the forward graph, each call will provide a SCC
~ O(m+n)
~ Meta-Graph : an acyclic graph composed of the SCC's of the original graph and edges connecting them.

Programming Assignment

~ Input : each edge is a row with two columns, the first column is the tail vertex and the second column is the head vertex
~ Output : sizes of the top 5 largest SCCs, as measured by number of nodes in the SCC.
~ Procedure :
1.) create several data structures
	~ adjacency list (each row is a vertex, contains vertices that it points to.)
	~ reverse adjacency list (the rows must match up with the adjacency list)
2.) DFS loop the rev-adj list, create 
	~ dictionary mapping vertices to finishing time
3.) DFS loop the adj-list in reverse fin-time order. create
	~ list of SCC sizes

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Week 5

Djikstra's Shortest Path Algorithm
~ directed or undirected graph
~ edge length non-negative
~ given a source vertex, computes length of shortest path between each vertex and the source
~ preprocess to eliminate unreachable vertices
~ Shortest Path Algorithms are used for Financial Transactions. Bellman-Ford
~ algo : loop over nodes, process distance of shortest path. each iteration take a node one edge away from a node we've already processed. pick the new node whose distance from source is shortest, measuring by the already calculated shortest paths to the nodes connecting to new nodes plus the length of the edge connecting to the new node  
~ 

Data Structures
~ organize data so that it can be accessed quickly.
~ choose the minimal data structure that supports all the operations you need

Heaps
~ each object has a key, that can be compared to another key 
~ Insertion : O(log n) 
~ Extract Min : O(log n)
~ Heapify : O(n)
~ Delete : O(log n)
~ good for repeatedly calculating minimums
~ Diskstra with Heaps is O(m log n)

Balanced Binary Search Tree
~ insert and delete to slow on sorted arrays O(n)
~ BBST can do O(log(n)) inserts and deletes. all other sorted array primitives become O(log(n))
~ BBST beats sorted arrays for dynamic arrays.
~ Binary Tree : leaves to the left are smaller than the parent, leaves to the right are larger
~ Height : from log2 n to 
~ Operations depend on height
~ In Order Traversal is cool. O(n) ordered printing
~ Deletion is cool. If node has 2 children, find its predecessor, swap it with its predecessor, then delete it again.
~ Select and Rank. Store size of each node's subtree

Red-Black Tree
~ one implementation balanced binary search tree.  
~ node is red or black
~ root is black
~ red's parent and children must be black
~ every root-NULL path has the same number of black nodes	


Week 5 Programming Assignment : Djikstra's Shortest Path
~ Algorithm
	Loop until all reachable* nodes are explored
	# loop over all edges whose tail node is explored but whose head node is unexplored
	# select the one with the lowest Djikstra score (edge length + shortest path to tail node)
	# add the head node to the explored nodes list.
	# the the distance to the head node to your shortest paths list
~ Data Structures
	# explored nodes list
	# adjacency list : tail_node head_node,distance head_node,distance etc.
	# shortest paths : list of shortest paths to different nodes

 OPTIONAL: For those of you seeking an additional challenge, try implementing the heap-based version. Note this requires a heap that supports deletions, and you'll probably need to maintain some kind of mapping between vertices and their positions in the heap.

#######################################################################################################################################################

Week 6

Hashtable
~ array indexed by keys. Really maps keys to array positions.
~ keys unique
~ insert, delete, lookup are all O(1)
~ objects unordered
~ needs to be properly implimented(enough buckets + good hash function), and not a pathological dataset
~ Useful for problems that require repeated lookups
~ Maintain an evolving subset of your Universe. 
~ number of buckets a constant factor times size of subset (can be resized)
~ hash function maps keys to positions in the array
~ Collision : two different keys getting hashed to the same position
~ Chaining : in the case of a collision, keep both elements at the same hash , use an array. Insert O(1), Lookup(O(n/m)) where m is number of buckets
~ Open Addressing : in case of collision, try another address. (saves space, but deletion takes more time)
~ Good hash function : spread data out as uniformly as possible. flipping independent coins is good
~ Good hash function : needs to be O(1)
~ Quick and dirty : map to a number, do mod a prime

Universal Hashing
~ every hash function has its kryptonite, its pathological data set
~ load : number of objects in the hash table / number of buckets in the hash table
~ load << 1 for open addressing, load <= 1 for chaining
~ dynamically change number of buckets in response to load
~ Pathological Data : for any hash function, we can generate a dataset whose keys all map to the same bucket
~ DoS can be done by constructing a pathological dataset. Crosby & Wallach USENIX 
~ solution : cryptographic hash function. infeasible to find a pathological dataset
~ solution : randomly choose between several hash functions. (Can't the attacker just try all pathological options?)

Bloom Filters
~ space efficient > hash table
~ can false positives
~ super fast inserts and lookups
~ doesn't store objects, just remembers if we've seen them. ie. maps each key to a single bit. actually to several, because we use k hash functions
~ false positives possible : says we've seen it even though we haven't
~ deletion not a first order operation, no deletion
~ ex. word checker : is this a word?
~ insert : run all k hash function on the input as set the hash addresses to 1
~ look up : run all k hash functions on the input and check if all k hash addresses are 1
~ trade off between space use and error rate

Programming Assignment 6

2-SUM
~ does it mean that x != y or that x can equal y but they need to be separate entries?


OPTIONAL CHALLENGE: If this problem is too easy for you, try implementing your own hash table for it. For example, you could compare performance under the chaining and open addressing approaches to resolving collisions.

OPTIONAL EXERCISE: Compare the performance achieved by heap-based and search-tree-based implementations of the algorithm.















































































































